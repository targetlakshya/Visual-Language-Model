{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01ad5848",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-20T16:26:59.205345Z",
     "iopub.status.busy": "2024-07-20T16:26:59.204973Z",
     "iopub.status.idle": "2024-07-20T16:26:59.996905Z",
     "shell.execute_reply": "2024-07-20T16:26:59.995966Z"
    },
    "papermill": {
     "duration": 0.804025,
     "end_time": "2024-07-20T16:26:59.999383",
     "exception": false,
     "start_time": "2024-07-20T16:26:59.195358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "base_dir= \"/kaggle/input/flickr-image-dataset/flickr30k_images/\"\n",
    "dataset = \"/kaggle/input/flickr-image-dataset/flickr30k_images/results.csv\"\n",
    "IMG_PATH = \"/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "744c516f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T16:27:00.016645Z",
     "iopub.status.busy": "2024-07-20T16:27:00.015636Z",
     "iopub.status.idle": "2024-07-20T16:27:28.186093Z",
     "shell.execute_reply": "2024-07-20T16:27:28.185085Z"
    },
    "papermill": {
     "duration": 28.18176,
     "end_time": "2024-07-20T16:27:28.188737",
     "exception": false,
     "start_time": "2024-07-20T16:27:00.006977",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_hub in /opt/conda/lib/python3.10/site-packages (0.16.1)\r\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow_hub) (1.26.4)\r\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow_hub) (3.20.3)\r\n",
      "Requirement already satisfied: tf-keras>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow_hub) (2.15.1)\r\n",
      "Requirement already satisfied: tensorflow<2.16,>=2.15 in /opt/conda/lib/python3.10/site-packages (from tf-keras>=2.14.1->tensorflow_hub) (2.15.0)\r\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (1.4.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (1.6.3)\r\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (23.5.26)\r\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (0.5.4)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (0.2.0)\r\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (3.10.0)\r\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (16.0.6)\r\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (0.2.0)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (3.3.0)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (21.3)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (69.0.3)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (1.16.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (2.4.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (4.9.0)\r\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (1.14.1)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (0.35.0)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (1.60.0)\r\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (2.15.1)\r\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (2.15.0)\r\n",
      "Collecting keras<2.16,>=2.15.0 (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub)\r\n",
      "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (0.42.0)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (2.26.1)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (1.2.0)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (3.5.2)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (2.32.3)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (0.7.2)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (3.0.3)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (3.1.1)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (4.2.4)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (0.3.0)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (4.9)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (1.3.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (2024.7.4)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (2.1.3)\r\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (0.5.1)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (3.2.2)\r\n",
      "Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: keras\r\n",
      "  Attempting uninstall: keras\r\n",
      "    Found existing installation: keras 3.4.1\r\n",
      "    Uninstalling keras-3.4.1:\r\n",
      "      Successfully uninstalled keras-3.4.1\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed keras-2.15.0\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-20 16:27:17.715745: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-20 16:27:17.715873: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-20 16:27:17.839713: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade tensorflow_hub\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba60f4f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T16:27:28.208022Z",
     "iopub.status.busy": "2024-07-20T16:27:28.206701Z",
     "iopub.status.idle": "2024-07-20T16:27:28.646634Z",
     "shell.execute_reply": "2024-07-20T16:27:28.645764Z"
    },
    "papermill": {
     "duration": 0.451958,
     "end_time": "2024-07-20T16:27:28.649160",
     "exception": false,
     "start_time": "2024-07-20T16:27:28.197202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b285b2e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T16:27:28.667856Z",
     "iopub.status.busy": "2024-07-20T16:27:28.666955Z",
     "iopub.status.idle": "2024-07-20T16:27:28.672008Z",
     "shell.execute_reply": "2024-07-20T16:27:28.671116Z"
    },
    "papermill": {
     "duration": 0.016753,
     "end_time": "2024-07-20T16:27:28.674355",
     "exception": false,
     "start_time": "2024-07-20T16:27:28.657602",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/flickr-image-dataset/flickr30k_images/results.csv\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc975ebf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T16:27:28.692485Z",
     "iopub.status.busy": "2024-07-20T16:27:28.691876Z",
     "iopub.status.idle": "2024-07-20T16:27:29.095656Z",
     "shell.execute_reply": "2024-07-20T16:27:29.094728Z"
    },
    "papermill": {
     "duration": 0.41561,
     "end_time": "2024-07-20T16:27:29.098192",
     "exception": false,
     "start_time": "2024-07-20T16:27:28.682582",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(dataset, delimiter='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66dc20ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T16:27:29.116769Z",
     "iopub.status.busy": "2024-07-20T16:27:29.116382Z",
     "iopub.status.idle": "2024-07-20T16:27:29.121793Z",
     "shell.execute_reply": "2024-07-20T16:27:29.120823Z"
    },
    "papermill": {
     "duration": 0.016934,
     "end_time": "2024-07-20T16:27:29.123770",
     "exception": false,
     "start_time": "2024-07-20T16:27:29.106836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm, tqdm_notebook\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "569202e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T16:27:29.141923Z",
     "iopub.status.busy": "2024-07-20T16:27:29.141541Z",
     "iopub.status.idle": "2024-07-20T16:27:29.160768Z",
     "shell.execute_reply": "2024-07-20T16:27:29.159790Z"
    },
    "papermill": {
     "duration": 0.030969,
     "end_time": "2024-07-20T16:27:29.163116",
     "exception": false,
     "start_time": "2024-07-20T16:27:29.132147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.fillna of             image_name  comment_number  \\\n",
       "0       1000092795.jpg               0   \n",
       "1       1000092795.jpg               1   \n",
       "2       1000092795.jpg               2   \n",
       "3       1000092795.jpg               3   \n",
       "4       1000092795.jpg               4   \n",
       "...                ...             ...   \n",
       "158910   998845445.jpg               0   \n",
       "158911   998845445.jpg               1   \n",
       "158912   998845445.jpg               2   \n",
       "158913   998845445.jpg               3   \n",
       "158914   998845445.jpg               4   \n",
       "\n",
       "                                                  comment  \n",
       "0        Two young guys with shaggy hair look at their...  \n",
       "1        Two young , White males are outside near many...  \n",
       "2        Two men in green shirts are standing in a yard .  \n",
       "3            A man in a blue shirt standing in a garden .  \n",
       "4                 Two friends enjoy time spent together .  \n",
       "...                                                   ...  \n",
       "158910   A man in shorts and a Hawaiian shirt leans ov...  \n",
       "158911   A young man hanging over the side of a boat ,...  \n",
       "158912   A man is leaning off of the side of a blue an...  \n",
       "158913   A man riding a small boat in a harbor , with ...  \n",
       "158914   A man on a moored blue and white boat with hi...  \n",
       "\n",
       "[158915 rows x 3 columns]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns\n",
    "df.fillna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10f04337",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T16:27:29.182333Z",
     "iopub.status.busy": "2024-07-20T16:27:29.181978Z",
     "iopub.status.idle": "2024-07-20T16:27:38.389318Z",
     "shell.execute_reply": "2024-07-20T16:27:38.388205Z"
    },
    "papermill": {
     "duration": 9.219507,
     "end_time": "2024-07-20T16:27:38.391976",
     "exception": false,
     "start_time": "2024-07-20T16:27:29.172469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_name_train=[]\n",
    "cap_train=[]\n",
    "for index, row in df.iterrows():\n",
    "    img_name_train.append(IMG_PATH+row['image_name'])\n",
    "    cap_train.append(str(row[' comment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "876876cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T16:27:38.412216Z",
     "iopub.status.busy": "2024-07-20T16:27:38.411815Z",
     "iopub.status.idle": "2024-07-20T16:27:38.420822Z",
     "shell.execute_reply": "2024-07-20T16:27:38.419731Z"
    },
    "papermill": {
     "duration": 0.020974,
     "end_time": "2024-07-20T16:27:38.422999",
     "exception": false,
     "start_time": "2024-07-20T16:27:38.402025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLICAS:  1\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "    # set: this is always the case on Kaggle.\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b46337c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T16:27:38.442243Z",
     "iopub.status.busy": "2024-07-20T16:27:38.441288Z",
     "iopub.status.idle": "2024-07-20T16:27:38.447308Z",
     "shell.execute_reply": "2024-07-20T16:27:38.446024Z"
    },
    "papermill": {
     "duration": 0.01788,
     "end_time": "2024-07-20T16:27:38.449381",
     "exception": false,
     "start_time": "2024-07-20T16:27:38.431501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158915 158915\n"
     ]
    }
   ],
   "source": [
    "print(len(img_name_train),len(cap_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df02e9bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T16:27:38.468087Z",
     "iopub.status.busy": "2024-07-20T16:27:38.467719Z",
     "iopub.status.idle": "2024-07-20T16:27:38.472877Z",
     "shell.execute_reply": "2024-07-20T16:27:38.471924Z"
    },
    "papermill": {
     "duration": 0.017201,
     "end_time": "2024-07-20T16:27:38.475220",
     "exception": false,
     "start_time": "2024-07-20T16:27:38.458019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/1000092795.jpg  Two young guys with shaggy hair look at their hands while hanging out in the yard .\n"
     ]
    }
   ],
   "source": [
    "print(img_name_train[0],cap_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f17d5e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T16:27:38.493913Z",
     "iopub.status.busy": "2024-07-20T16:27:38.493525Z",
     "iopub.status.idle": "2024-07-20T16:29:17.850032Z",
     "shell.execute_reply": "2024-07-20T16:29:17.849106Z"
    },
    "papermill": {
     "duration": 99.368893,
     "end_time": "2024-07-20T16:29:17.852783",
     "exception": false,
     "start_time": "2024-07-20T16:27:38.483890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalization(embeds):\n",
    "    norms = np.linalg.norm(embeds, 2, axis=1, keepdims=True)\n",
    "    return embeds/norms\n",
    "preprocessor = hub.KerasLayer(\n",
    "    \"https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-preprocess/2\")\n",
    "encoder = hub.KerasLayer(\n",
    "    \"https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-base-br/1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3aecee83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T16:29:17.872482Z",
     "iopub.status.busy": "2024-07-20T16:29:17.871528Z",
     "iopub.status.idle": "2024-07-20T16:29:17.876544Z",
     "shell.execute_reply": "2024-07-20T16:29:17.875404Z"
    },
    "papermill": {
     "duration": 0.016851,
     "end_time": "2024-07-20T16:29:17.878710",
     "exception": false,
     "start_time": "2024-07-20T16:29:17.861859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=5\n",
    "target_size=(128,128)\n",
    "embedding_dim=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2a0aa0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T16:29:17.897178Z",
     "iopub.status.busy": "2024-07-20T16:29:17.896844Z",
     "iopub.status.idle": "2024-07-20T16:29:17.903594Z",
     "shell.execute_reply": "2024-07-20T16:29:17.902619Z"
    },
    "papermill": {
     "duration": 0.018457,
     "end_time": "2024-07-20T16:29:17.905743",
     "exception": false,
     "start_time": "2024-07-20T16:29:17.887286",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decode_image(filename, label=None, image_size=(target_size[0],target_size[1])):\n",
    "    means = [0.485, 0.456, 0.406]\n",
    "    stds = [0.229, 0.224, 0.225]\n",
    "    \n",
    "    bits = tf.io.read_file(filename)\n",
    "    image = tf.image.decode_jpeg(bits, channels=3)\n",
    "    \n",
    "    image = (tf.cast(image, tf.float32) / 255.0)\n",
    "    image = (image - means) / stds # for qubvel EfficientNet\n",
    "    \n",
    "    image = tf.image.resize(image, image_size)\n",
    "    \n",
    "    if label is None:\n",
    "        return image\n",
    "    else:\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c4cb0a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T16:29:17.924281Z",
     "iopub.status.busy": "2024-07-20T16:29:17.923930Z",
     "iopub.status.idle": "2024-07-20T16:29:17.929126Z",
     "shell.execute_reply": "2024-07-20T16:29:17.928060Z"
    },
    "papermill": {
     "duration": 0.016926,
     "end_time": "2024-07-20T16:29:17.931191",
     "exception": false,
     "start_time": "2024-07-20T16:29:17.914265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_augment(image, label=None):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    \n",
    "    if label is None:\n",
    "        return image\n",
    "    else:\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c797b1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T16:29:17.950221Z",
     "iopub.status.busy": "2024-07-20T16:29:17.949448Z",
     "iopub.status.idle": "2024-07-20T16:29:17.956664Z",
     "shell.execute_reply": "2024-07-20T16:29:17.955678Z"
    },
    "papermill": {
     "duration": 0.019059,
     "end_time": "2024-07-20T16:29:17.958784",
     "exception": false,
     "start_time": "2024-07-20T16:29:17.939725",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_training_dataset():\n",
    "    train_dataset = (\n",
    "        tf.data.Dataset\n",
    "        .from_tensor_slices((img_name_train, cap_train))\n",
    "        .map(decode_image, num_parallel_calls=10)\n",
    "        .cache()\n",
    "        .map(data_augment, num_parallel_calls=10)\n",
    "        .repeat() # Maybe not repeat in custom training (so when and how??) <-- the current version is bug because it repeat indefinitely\n",
    "        .shuffle(BATCH_SIZE*8, reshuffle_each_iteration=True)\n",
    "        .batch(BATCH_SIZE, drop_remainder=False)\n",
    "        .prefetch(10)\n",
    "    )\n",
    "    return strategy.experimental_distribute_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d564ac27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T16:29:17.978050Z",
     "iopub.status.busy": "2024-07-20T16:29:17.977142Z",
     "iopub.status.idle": "2024-07-20T16:29:46.450395Z",
     "shell.execute_reply": "2024-07-20T16:29:46.449356Z"
    },
    "papermill": {
     "duration": 28.485453,
     "end_time": "2024-07-20T16:29:46.452865",
     "exception": false,
     "start_time": "2024-07-20T16:29:17.967412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "imageEncoderLayer=hub.KerasLayer(\"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_m/feature_vector/2\",\n",
    "                   trainable=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c788f814",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T16:29:46.472310Z",
     "iopub.status.busy": "2024-07-20T16:29:46.471951Z",
     "iopub.status.idle": "2024-07-20T16:29:46.477985Z",
     "shell.execute_reply": "2024-07-20T16:29:46.476982Z"
    },
    "papermill": {
     "duration": 0.017949,
     "end_time": "2024-07-20T16:29:46.479913",
     "exception": false,
     "start_time": "2024-07-20T16:29:46.461964",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VisionEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(VisionEncoder,self).__init__()\n",
    "        self.encoder=imageEncoderLayer\n",
    "        self.ds=tf.keras.layers.Dense(embedding_dim,activation=\"relu\")\n",
    "    def call(self, x):\n",
    "        x=self.encoder(x)\n",
    "        x=self.ds(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49e18de5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T16:29:46.499096Z",
     "iopub.status.busy": "2024-07-20T16:29:46.498738Z",
     "iopub.status.idle": "2024-07-20T16:29:46.505265Z",
     "shell.execute_reply": "2024-07-20T16:29:46.504386Z"
    },
    "papermill": {
     "duration": 0.018443,
     "end_time": "2024-07-20T16:29:46.507320",
     "exception": false,
     "start_time": "2024-07-20T16:29:46.488877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TextEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self,preprocessor,encoder):\n",
    "        super(TextEncoder,self).__init__()\n",
    "        self.preprocessor=preprocessor\n",
    "        self.encoder=encoder\n",
    "        self.ds=tf.keras.layers.Dense(embedding_dim,activation=\"relu\")\n",
    "    def call(self, x):\n",
    "        x=self.preprocessor(x)\n",
    "        x=self.encoder(x)['default']\n",
    "        x=normalization(x)\n",
    "        x=self.ds(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "263d5985",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T16:29:46.526197Z",
     "iopub.status.busy": "2024-07-20T16:29:46.525826Z",
     "iopub.status.idle": "2024-07-20T16:29:47.607054Z",
     "shell.execute_reply": "2024-07-20T16:29:47.606105Z"
    },
    "papermill": {
     "duration": 1.093445,
     "end_time": "2024-07-20T16:29:47.609481",
     "exception": false,
     "start_time": "2024-07-20T16:29:46.516036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds=get_training_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8bd4c5b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T16:29:47.628603Z",
     "iopub.status.busy": "2024-07-20T16:29:47.628219Z",
     "iopub.status.idle": "2024-07-20T16:29:47.632824Z",
     "shell.execute_reply": "2024-07-20T16:29:47.631786Z"
    },
    "papermill": {
     "duration": 0.016585,
     "end_time": "2024-07-20T16:29:47.634963",
     "exception": false,
     "start_time": "2024-07-20T16:29:47.618378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_heads=10\n",
    "LR=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "430236ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T16:29:47.654287Z",
     "iopub.status.busy": "2024-07-20T16:29:47.653918Z",
     "iopub.status.idle": "2024-07-20T16:29:47.662102Z",
     "shell.execute_reply": "2024-07-20T16:29:47.661120Z"
    },
    "papermill": {
     "duration": 0.020611,
     "end_time": "2024-07-20T16:29:47.664297",
     "exception": false,
     "start_time": "2024-07-20T16:29:47.643686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CrossAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.query=tf.keras.layers.Dense(embedding_dim,activation=\"relu\")\n",
    "        self.key=tf.keras.layers.Dense(embedding_dim,activation=\"relu\")\n",
    "        self.value=tf.keras.layers.Dense(embedding_dim,activation=\"relu\")\n",
    "        self.outputs=tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        vision_inputs,text_inputs=inputs\n",
    "        query_inputs=self.query(vision_inputs)\n",
    "        key_inputs=self.key(text_inputs)\n",
    "        value_inputs=self.value(text_inputs)\n",
    "        attention_scores = tf.matmul(query_inputs,key_inputs, transpose_b=True)\n",
    "        attention_scores = tf.nn.tanh(attention_scores)\n",
    "        attention_weights = tf.nn.softmax(attention_scores, axis=-1)\n",
    "        attended = tf.matmul(attention_weights,value_inputs)\n",
    "        output = tf.concat([text_inputs,attended],axis=-1)\n",
    "        output=self.outputs(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77010005",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T16:29:47.683513Z",
     "iopub.status.busy": "2024-07-20T16:29:47.682625Z",
     "iopub.status.idle": "2024-07-20T16:29:47.689591Z",
     "shell.execute_reply": "2024-07-20T16:29:47.688627Z"
    },
    "papermill": {
     "duration": 0.018684,
     "end_time": "2024-07-20T16:29:47.691649",
     "exception": false,
     "start_time": "2024-07-20T16:29:47.672965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CrossAttentionEncoder(tf.keras.Model):\n",
    "    def __init__(self,visionEncoder,textEncoder):\n",
    "        super(CrossAttentionEncoder,self).__init__()\n",
    "        self.visionEncoder=visionEncoder\n",
    "        self.textEncoder=textEncoder\n",
    "        self.cross_attention=CrossAttention()\n",
    "        self.cross_attention2=CrossAttention()\n",
    "        self.similarity=tf.keras.losses.CosineSimilarity(axis=1)\n",
    "    def call(self,inputs):\n",
    "        vision_input,text_input=inputs\n",
    "#         print(vision_input,text_input)\n",
    "        vision_input=self.visionEncoder(vision_input)\n",
    "        text_input=self.textEncoder(text_input)\n",
    "        output1=self.cross_attention((vision_input,text_input))\n",
    "        output2=self.cross_attention2((text_input,vision_input))\n",
    "        return output1,output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91f4a5f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T16:29:47.711300Z",
     "iopub.status.busy": "2024-07-20T16:29:47.710454Z",
     "iopub.status.idle": "2024-07-20T16:29:48.403014Z",
     "shell.execute_reply": "2024-07-20T16:29:48.402043Z"
    },
    "papermill": {
     "duration": 0.704622,
     "end_time": "2024-07-20T16:29:48.405221",
     "exception": false,
     "start_time": "2024-07-20T16:29:47.700599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d09322cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T16:29:48.424428Z",
     "iopub.status.busy": "2024-07-20T16:29:48.423607Z",
     "iopub.status.idle": "2024-07-20T16:29:48.460375Z",
     "shell.execute_reply": "2024-07-20T16:29:48.459299Z"
    },
    "papermill": {
     "duration": 0.048781,
     "end_time": "2024-07-20T16:29:48.462727",
     "exception": false,
     "start_time": "2024-07-20T16:29:48.413946",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# with strategy.scope():\n",
    "textEncoder = TextEncoder(preprocessor,encoder)\n",
    "ImageEncoder=VisionEncoder()\n",
    "transformer=CrossAttentionEncoder(ImageEncoder,textEncoder)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LR)\n",
    "loss_object = tf.keras.losses.CosineSimilarity()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07b82e31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T16:29:48.482066Z",
     "iopub.status.busy": "2024-07-20T16:29:48.481329Z",
     "iopub.status.idle": "2024-07-20T16:29:48.488122Z",
     "shell.execute_reply": "2024-07-20T16:29:48.487172Z"
    },
    "papermill": {
     "duration": 0.018733,
     "end_time": "2024-07-20T16:29:48.490303",
     "exception": false,
     "start_time": "2024-07-20T16:29:48.471570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_step(img_tensor, text):\n",
    "    loss=0\n",
    "    hidden=transformer.reset_states()\n",
    "    with tf.GradientTape() as tape:\n",
    "        emb_a, emb_b=transformer((img_tensor,text))\n",
    "        loss+=1-loss_object(emb_a,emb_b)\n",
    "    total_loss=loss\n",
    "    trainable_variables=transformer.trainable_variables\n",
    "    gradient=tape.gradient(loss,trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradient,trainable_variables))\n",
    "    return loss, total_loss\n",
    "def distributed_train_step(inputs):\n",
    "    (images, labels) = inputs\n",
    "    loss = strategy.run(train_step, args=(images, labels))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dca0cd6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T16:29:48.509800Z",
     "iopub.status.busy": "2024-07-20T16:29:48.508914Z",
     "iopub.status.idle": "2024-07-20T17:43:31.389443Z",
     "shell.execute_reply": "2024-07-20T17:43:31.388475Z"
    },
    "papermill": {
     "duration": 4422.892551,
     "end_time": "2024-07-20T17:43:31.391623",
     "exception": false,
     "start_time": "2024-07-20T16:29:48.499072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 1 Loss 1.0274\n",
      "Epoch 1 Batch 2 Loss 0.6361\n",
      "Epoch 1 Batch 3 Loss 0.4607\n",
      "Epoch 1 Batch 4 Loss 0.3337\n",
      "Epoch 1 Batch 5 Loss 0.2656\n",
      "Epoch 1 Batch 6 Loss 0.1611\n",
      "Epoch 1 Batch 7 Loss 0.1861\n",
      "Epoch 1 Batch 8 Loss 0.1215\n",
      "Epoch 1 Batch 9 Loss 0.0919\n",
      "Epoch 1 Batch 10 Loss 0.0963\n",
      "Epoch 1 Batch 11 Loss 0.1134\n",
      "Epoch 1 Batch 12 Loss 0.0675\n",
      "Epoch 1 Batch 13 Loss 0.0714\n",
      "Epoch 1 Batch 14 Loss 0.0559\n",
      "Epoch 1 Batch 15 Loss 0.0593\n",
      "Epoch 1 Batch 16 Loss 0.0405\n",
      "Epoch 1 Batch 17 Loss 0.0383\n",
      "Epoch 1 Batch 18 Loss 0.0402\n",
      "Epoch 1 Batch 19 Loss 0.0319\n",
      "Epoch 1 Batch 20 Loss 0.0248\n",
      "Epoch 1 Batch 21 Loss 0.0851\n",
      "Epoch 1 Batch 22 Loss 0.0241\n",
      "Epoch 1 Batch 23 Loss 0.0217\n",
      "Epoch 1 Batch 24 Loss 0.0162\n",
      "Epoch 1 Batch 25 Loss 0.0552\n",
      "Epoch 1 Batch 26 Loss 0.0309\n",
      "Epoch 1 Batch 27 Loss 0.0393\n",
      "Epoch 1 Batch 28 Loss 0.0409\n",
      "Epoch 1 Batch 29 Loss 0.0155\n",
      "Epoch 1 Batch 30 Loss 0.0315\n",
      "Epoch 1 Batch 31 Loss 0.0292\n",
      "Epoch 1 Batch 32 Loss 0.0173\n",
      "Epoch 1 Batch 33 Loss 0.0602\n",
      "Epoch 1 Batch 34 Loss 0.0247\n",
      "Epoch 1 Batch 35 Loss 0.0172\n",
      "Epoch 1 Batch 36 Loss 0.0232\n",
      "Epoch 1 Batch 37 Loss 0.0185\n",
      "Epoch 1 Batch 38 Loss 0.0199\n",
      "Epoch 1 Batch 39 Loss 0.0130\n",
      "Epoch 1 Batch 40 Loss 0.0155\n",
      "Epoch 1 Batch 41 Loss 0.0089\n",
      "Epoch 1 Batch 42 Loss 0.0091\n",
      "Epoch 1 Batch 43 Loss 0.0095\n",
      "Epoch 1 Batch 44 Loss 0.0075\n",
      "Epoch 1 Batch 45 Loss 0.0071\n",
      "Epoch 1 Batch 46 Loss 0.0116\n",
      "Epoch 1 Batch 47 Loss 0.0113\n",
      "Epoch 1 Batch 48 Loss 0.0071\n",
      "Epoch 1 Batch 49 Loss 0.0117\n",
      "Epoch 1 Batch 50 Loss 0.0126\n",
      "Epoch 1 Batch 51 Loss 0.0080\n",
      "Epoch 1 Batch 52 Loss 0.0117\n",
      "Epoch 1 Batch 53 Loss 0.0077\n",
      "Epoch 1 Batch 54 Loss 0.0077\n",
      "Epoch 1 Batch 55 Loss 0.0068\n",
      "Epoch 1 Batch 56 Loss 0.0081\n",
      "Epoch 1 Batch 57 Loss 0.0091\n",
      "Epoch 1 Batch 58 Loss 0.0065\n",
      "Epoch 1 Batch 59 Loss 0.0059\n",
      "Epoch 1 Batch 60 Loss 0.0072\n",
      "Epoch 1 Batch 61 Loss 0.0073\n",
      "Epoch 1 Batch 62 Loss 0.0060\n",
      "Epoch 1 Batch 63 Loss 0.0059\n",
      "Epoch 1 Batch 64 Loss 0.0074\n",
      "Epoch 1 Batch 65 Loss 0.0072\n",
      "Epoch 1 Batch 66 Loss 0.0047\n",
      "Epoch 1 Batch 67 Loss 0.0049\n",
      "Epoch 1 Batch 68 Loss 0.0071\n",
      "Epoch 1 Batch 69 Loss 0.0068\n",
      "Epoch 1 Batch 70 Loss 0.0091\n",
      "Epoch 1 Batch 71 Loss 0.0059\n",
      "Epoch 1 Batch 72 Loss 0.0053\n",
      "Epoch 1 Batch 73 Loss 0.0053\n",
      "Epoch 1 Batch 74 Loss 0.0043\n",
      "Epoch 1 Batch 75 Loss 0.0053\n",
      "Epoch 1 Batch 76 Loss 0.0048\n",
      "Epoch 1 Batch 77 Loss 0.0071\n",
      "Epoch 1 Batch 78 Loss 0.0132\n",
      "Epoch 1 Batch 79 Loss 0.0100\n",
      "Epoch 1 Batch 80 Loss 0.0070\n",
      "Epoch 1 Batch 81 Loss 0.0048\n",
      "Epoch 1 Batch 82 Loss 0.0050\n",
      "Epoch 1 Batch 83 Loss 0.0042\n",
      "Epoch 1 Batch 84 Loss 0.0064\n",
      "Epoch 1 Batch 85 Loss 0.0069\n",
      "Epoch 1 Batch 86 Loss 0.0046\n",
      "Epoch 1 Batch 87 Loss 0.0041\n",
      "Epoch 1 Batch 88 Loss 0.0059\n",
      "Epoch 1 Batch 89 Loss 0.0046\n",
      "Epoch 1 Batch 90 Loss 0.0067\n",
      "Epoch 1 Batch 91 Loss 0.0042\n",
      "Epoch 1 Batch 92 Loss 0.0040\n",
      "Epoch 1 Batch 93 Loss 0.0111\n",
      "Epoch 1 Batch 94 Loss 0.0040\n",
      "Epoch 1 Batch 95 Loss 0.0064\n",
      "Epoch 1 Batch 96 Loss 0.0051\n",
      "Epoch 1 Batch 97 Loss 0.0058\n",
      "Epoch 1 Batch 98 Loss 0.0052\n",
      "Epoch 1 Batch 99 Loss 0.0043\n",
      "Epoch 1 Batch 100 Loss 0.0051\n",
      "Epoch 1 Batch 101 Loss 0.0060\n",
      "Epoch 1 Batch 102 Loss 0.0046\n",
      "Epoch 1 Batch 103 Loss 0.0043\n",
      "Epoch 1 Batch 104 Loss 0.0045\n",
      "Epoch 1 Batch 105 Loss 0.0045\n",
      "Epoch 1 Batch 106 Loss 0.0034\n",
      "Epoch 1 Batch 107 Loss 0.0031\n",
      "Epoch 1 Batch 108 Loss 0.0055\n",
      "Epoch 1 Batch 109 Loss 0.0029\n",
      "Epoch 1 Batch 110 Loss 0.0038\n",
      "Epoch 1 Batch 111 Loss 0.0048\n",
      "Epoch 1 Batch 112 Loss 0.0032\n",
      "Epoch 1 Batch 113 Loss 0.0033\n",
      "Epoch 1 Batch 114 Loss 0.0037\n",
      "Epoch 1 Batch 115 Loss 0.0049\n",
      "Epoch 1 Batch 116 Loss 0.0032\n",
      "Epoch 1 Batch 117 Loss 0.0058\n",
      "Epoch 1 Batch 118 Loss 0.0070\n",
      "Epoch 1 Batch 119 Loss 0.0036\n",
      "Epoch 1 Batch 120 Loss 0.0050\n",
      "Epoch 1 Batch 121 Loss 0.0035\n",
      "Epoch 1 Batch 122 Loss 0.0033\n",
      "Epoch 1 Batch 123 Loss 0.0042\n",
      "Epoch 1 Batch 124 Loss 0.0032\n",
      "Epoch 1 Batch 125 Loss 0.0040\n",
      "Epoch 1 Batch 126 Loss 0.0025\n",
      "Epoch 1 Batch 127 Loss 0.0044\n",
      "Epoch 1 Batch 128 Loss 0.0042\n",
      "Epoch 1 Batch 129 Loss 0.0028\n",
      "Epoch 1 Batch 130 Loss 0.0030\n",
      "Epoch 1 Batch 131 Loss 0.0035\n",
      "Epoch 1 Batch 132 Loss 0.0051\n",
      "Epoch 1 Batch 133 Loss 0.0034\n",
      "Epoch 1 Batch 134 Loss 0.0019\n",
      "Epoch 1 Batch 135 Loss 0.0026\n",
      "Epoch 1 Batch 136 Loss 0.0051\n",
      "Epoch 1 Batch 137 Loss 0.0029\n",
      "Epoch 1 Batch 138 Loss 0.0047\n",
      "Epoch 1 Batch 139 Loss 0.0037\n",
      "Epoch 1 Batch 140 Loss 0.0035\n",
      "Epoch 1 Batch 141 Loss 0.0028\n",
      "Epoch 1 Batch 142 Loss 0.0030\n",
      "Epoch 1 Batch 143 Loss 0.0027\n",
      "Epoch 1 Batch 144 Loss 0.0031\n",
      "Epoch 1 Batch 145 Loss 0.0040\n",
      "Epoch 1 Batch 146 Loss 0.0047\n",
      "Epoch 1 Batch 147 Loss 0.0575\n",
      "Epoch 1 Batch 148 Loss 0.0369\n",
      "Epoch 1 Batch 149 Loss 0.0036\n",
      "Epoch 1 Batch 150 Loss 0.0218\n",
      "Epoch 1 Batch 151 Loss 0.0076\n",
      "Epoch 1 Batch 152 Loss 0.0048\n",
      "Epoch 1 Batch 153 Loss 0.0041\n",
      "Epoch 1 Batch 154 Loss 0.0051\n",
      "Epoch 1 Batch 155 Loss 0.0043\n",
      "Epoch 1 Batch 156 Loss 0.0059\n",
      "Epoch 1 Batch 157 Loss 0.0038\n",
      "Epoch 1 Batch 158 Loss 0.0045\n",
      "Epoch 1 Batch 159 Loss 0.0038\n",
      "Epoch 1 Batch 160 Loss 0.0024\n",
      "Epoch 1 Batch 161 Loss 0.0020\n",
      "Epoch 1 Batch 162 Loss 0.0027\n",
      "Epoch 1 Batch 163 Loss 0.0030\n",
      "Epoch 1 Batch 164 Loss 0.0030\n",
      "Epoch 1 Batch 165 Loss 0.0044\n",
      "Epoch 1 Batch 166 Loss 0.0041\n",
      "Epoch 1 Batch 167 Loss 0.0024\n",
      "Epoch 1 Batch 168 Loss 0.0024\n",
      "Epoch 1 Batch 169 Loss 0.0028\n",
      "Epoch 1 Batch 170 Loss 0.0063\n",
      "Epoch 1 Batch 171 Loss 0.0042\n",
      "Epoch 1 Batch 172 Loss 0.0028\n",
      "Epoch 1 Batch 173 Loss 0.0033\n",
      "Epoch 1 Batch 174 Loss 0.0025\n",
      "Epoch 1 Batch 175 Loss 0.0022\n",
      "Epoch 1 Batch 176 Loss 0.0023\n",
      "Epoch 1 Batch 177 Loss 0.0027\n",
      "Epoch 1 Batch 178 Loss 0.0034\n",
      "Epoch 1 Batch 179 Loss 0.0021\n",
      "Epoch 1 Batch 180 Loss 0.0027\n",
      "Epoch 1 Batch 181 Loss 0.0042\n",
      "Epoch 1 Batch 182 Loss 0.0026\n",
      "Epoch 1 Batch 183 Loss 0.0029\n",
      "Epoch 1 Batch 184 Loss 0.0030\n",
      "Epoch 1 Batch 185 Loss 0.0092\n",
      "Epoch 1 Batch 186 Loss 0.0018\n",
      "Epoch 1 Batch 187 Loss 0.0037\n",
      "Epoch 1 Batch 188 Loss 0.0030\n",
      "Epoch 1 Batch 189 Loss 0.0030\n",
      "Epoch 1 Batch 190 Loss 0.0040\n",
      "Epoch 1 Batch 191 Loss 0.0031\n",
      "Epoch 1 Batch 192 Loss 0.0019\n",
      "Epoch 1 Batch 193 Loss 0.0027\n",
      "Epoch 1 Batch 194 Loss 0.0029\n",
      "Epoch 1 Batch 195 Loss 0.0023\n",
      "Epoch 1 Batch 196 Loss 0.0025\n",
      "Epoch 1 Batch 197 Loss 0.0036\n",
      "Epoch 1 Batch 198 Loss 0.0036\n",
      "Epoch 1 Batch 199 Loss 0.0020\n",
      "Epoch 1 Batch 200 Loss 0.0032\n",
      "Epoch 1 Batch 201 Loss 0.0025\n",
      "Epoch 1 Batch 202 Loss 0.0024\n",
      "Epoch 1 Batch 203 Loss 0.0020\n",
      "Epoch 1 Batch 204 Loss 0.0021\n",
      "Epoch 1 Batch 205 Loss 0.0020\n",
      "Epoch 1 Batch 206 Loss 0.0024\n",
      "Epoch 1 Batch 207 Loss 0.0023\n",
      "Epoch 1 Batch 208 Loss 0.0013\n",
      "Epoch 1 Batch 209 Loss 0.0042\n",
      "Epoch 1 Batch 210 Loss 0.0019\n",
      "Epoch 1 Batch 211 Loss 0.0016\n",
      "Epoch 1 Batch 212 Loss 0.0020\n",
      "Epoch 1 Batch 213 Loss 0.0015\n",
      "Epoch 1 Batch 214 Loss 0.0020\n",
      "Epoch 1 Batch 215 Loss 0.0024\n",
      "Epoch 1 Batch 216 Loss 0.0026\n",
      "Epoch 1 Batch 217 Loss 0.0017\n",
      "Epoch 1 Batch 218 Loss 0.0019\n",
      "Epoch 1 Batch 219 Loss 0.0019\n",
      "Epoch 1 Batch 220 Loss 0.0022\n",
      "Epoch 1 Batch 221 Loss 0.0016\n",
      "Epoch 1 Batch 222 Loss 0.0026\n",
      "Epoch 1 Batch 223 Loss 0.0021\n",
      "Epoch 1 Batch 224 Loss 0.0018\n",
      "Epoch 1 Batch 225 Loss 0.0020\n",
      "Epoch 1 Batch 226 Loss 0.0023\n",
      "Epoch 1 Batch 227 Loss 0.0021\n",
      "Epoch 1 Batch 228 Loss 0.0021\n",
      "Epoch 1 Batch 229 Loss 0.0018\n",
      "Epoch 1 Batch 230 Loss 0.0014\n",
      "Epoch 1 Batch 231 Loss 0.0021\n",
      "Epoch 1 Batch 232 Loss 0.0017\n",
      "Epoch 1 Batch 233 Loss 0.0020\n",
      "Epoch 1 Batch 234 Loss 0.0026\n",
      "Epoch 1 Batch 235 Loss 0.0019\n",
      "Epoch 1 Batch 236 Loss 0.0035\n",
      "Epoch 1 Batch 237 Loss 0.0025\n",
      "Epoch 1 Batch 238 Loss 0.0020\n",
      "Epoch 1 Batch 239 Loss 0.0026\n",
      "Epoch 1 Batch 240 Loss 0.0025\n",
      "Epoch 1 Batch 241 Loss 0.0024\n",
      "Epoch 1 Batch 242 Loss 0.0016\n",
      "Epoch 1 Batch 243 Loss 0.0016\n",
      "Epoch 1 Batch 244 Loss 0.0024\n",
      "Epoch 1 Batch 245 Loss 0.0031\n",
      "Epoch 1 Batch 246 Loss 0.0017\n",
      "Epoch 1 Batch 247 Loss 0.0016\n",
      "Epoch 1 Batch 248 Loss 0.0016\n",
      "Epoch 1 Batch 249 Loss 0.0016\n",
      "Epoch 1 Batch 250 Loss 0.0022\n",
      "Epoch 1 Batch 251 Loss 0.0016\n",
      "Epoch 1 Batch 252 Loss 0.0023\n",
      "Epoch 1 Batch 253 Loss 0.0018\n",
      "Epoch 1 Batch 254 Loss 0.0019\n",
      "Epoch 1 Batch 255 Loss 0.0015\n",
      "Epoch 1 Batch 256 Loss 0.0013\n",
      "Epoch 1 Batch 257 Loss 0.0014\n",
      "Epoch 1 Batch 258 Loss 0.0017\n",
      "Epoch 1 Batch 259 Loss 0.0014\n",
      "Epoch 1 Batch 260 Loss 0.0011\n",
      "Epoch 1 Batch 261 Loss 0.0018\n",
      "Epoch 1 Batch 262 Loss 0.0017\n",
      "Epoch 1 Batch 263 Loss 0.0016\n",
      "Epoch 1 Batch 264 Loss 0.0014\n",
      "Epoch 1 Batch 265 Loss 0.0012\n",
      "Epoch 1 Batch 266 Loss 0.0012\n",
      "Epoch 1 Batch 267 Loss 0.0016\n",
      "Epoch 1 Batch 268 Loss 0.0013\n",
      "Epoch 1 Batch 269 Loss 0.0013\n",
      "Epoch 1 Batch 270 Loss 0.0017\n",
      "Epoch 1 Batch 271 Loss 0.0013\n",
      "Epoch 1 Batch 272 Loss 0.0018\n",
      "Epoch 1 Batch 273 Loss 0.0012\n",
      "Epoch 1 Batch 274 Loss 0.0016\n",
      "Epoch 1 Batch 275 Loss 0.0011\n",
      "Epoch 1 Batch 276 Loss 0.0020\n",
      "Epoch 1 Batch 277 Loss 0.0012\n",
      "Epoch 1 Batch 278 Loss 0.0013\n",
      "Epoch 1 Batch 279 Loss 0.0015\n",
      "Epoch 1 Batch 280 Loss 0.0015\n",
      "Epoch 1 Batch 281 Loss 0.0013\n",
      "Epoch 1 Batch 282 Loss 0.0012\n",
      "Epoch 1 Batch 283 Loss 0.0012\n",
      "Epoch 1 Batch 284 Loss 0.0014\n",
      "Epoch 1 Batch 285 Loss 0.0012\n",
      "Epoch 1 Batch 286 Loss 0.0024\n",
      "Epoch 1 Batch 287 Loss 0.0012\n",
      "Epoch 1 Batch 288 Loss 0.0009\n",
      "Epoch 1 Batch 289 Loss 0.0011\n",
      "Epoch 1 Batch 290 Loss 0.0014\n",
      "Epoch 1 Batch 291 Loss 0.0018\n",
      "Epoch 1 Batch 292 Loss 0.0012\n",
      "Epoch 1 Batch 293 Loss 0.0021\n",
      "Epoch 1 Batch 294 Loss 0.0016\n",
      "Epoch 1 Batch 295 Loss 0.0014\n",
      "Epoch 1 Batch 296 Loss 0.0012\n",
      "Epoch 1 Batch 297 Loss 0.0012\n",
      "Epoch 1 Batch 298 Loss 0.0013\n",
      "Epoch 1 Batch 299 Loss 0.0015\n",
      "Epoch 1 Batch 300 Loss 0.0018\n",
      "Epoch 1 Batch 301 Loss 0.0010\n",
      "Epoch 1 Batch 302 Loss 0.0009\n",
      "Epoch 1 Batch 303 Loss 0.0010\n",
      "Epoch 1 Batch 304 Loss 0.0010\n",
      "Epoch 1 Batch 305 Loss 0.0011\n",
      "Epoch 1 Batch 306 Loss 0.0008\n",
      "Epoch 1 Batch 307 Loss 0.0014\n",
      "Epoch 1 Batch 308 Loss 0.0013\n",
      "Epoch 1 Batch 309 Loss 0.0010\n",
      "Epoch 1 Batch 310 Loss 0.0008\n",
      "Epoch 1 Batch 311 Loss 0.0014\n",
      "Epoch 1 Batch 312 Loss 0.0015\n",
      "Epoch 1 Batch 313 Loss 0.0011\n",
      "Epoch 1 Batch 314 Loss 0.0016\n",
      "Epoch 1 Batch 315 Loss 0.0011\n",
      "Epoch 1 Batch 316 Loss 0.0081\n",
      "Epoch 1 Batch 317 Loss 0.0014\n",
      "Epoch 1 Batch 318 Loss 0.0041\n",
      "Epoch 1 Batch 319 Loss 0.0015\n",
      "Epoch 1 Batch 320 Loss 0.0010\n",
      "Epoch 1 Batch 321 Loss 0.0019\n",
      "Epoch 1 Batch 322 Loss 0.0013\n",
      "Epoch 1 Batch 323 Loss 0.0013\n",
      "Epoch 1 Batch 324 Loss 0.0010\n",
      "Epoch 1 Batch 325 Loss 0.0019\n",
      "Epoch 1 Batch 326 Loss 0.0240\n",
      "Epoch 1 Batch 327 Loss 0.0016\n",
      "Epoch 1 Batch 328 Loss 0.0111\n",
      "Epoch 1 Batch 329 Loss 0.0015\n",
      "Epoch 1 Batch 330 Loss 0.0016\n",
      "Epoch 1 Batch 331 Loss 0.0162\n",
      "Epoch 1 Batch 332 Loss 0.0014\n",
      "Epoch 1 Batch 333 Loss 0.0019\n",
      "Epoch 1 Batch 334 Loss 0.0018\n",
      "Epoch 1 Batch 335 Loss 0.0012\n",
      "Epoch 1 Batch 336 Loss 0.0011\n",
      "Epoch 1 Batch 337 Loss 0.0013\n",
      "Epoch 1 Batch 338 Loss 0.0011\n",
      "Epoch 1 Batch 339 Loss 0.0009\n",
      "Epoch 1 Batch 340 Loss 0.0016\n",
      "Epoch 1 Batch 341 Loss 0.0015\n",
      "Epoch 1 Batch 342 Loss 0.0010\n",
      "Epoch 1 Batch 343 Loss 0.0017\n",
      "Epoch 1 Batch 344 Loss 0.0009\n",
      "Epoch 1 Batch 345 Loss 0.0010\n",
      "Epoch 1 Batch 346 Loss 0.0009\n",
      "Epoch 1 Batch 347 Loss 0.0013\n",
      "Epoch 1 Batch 348 Loss 0.0008\n",
      "Epoch 1 Batch 349 Loss 0.0009\n",
      "Epoch 1 Batch 350 Loss 0.0010\n",
      "Epoch 1 Batch 351 Loss 0.0013\n",
      "Epoch 1 Batch 352 Loss 0.0009\n",
      "Epoch 1 Batch 353 Loss 0.0013\n",
      "Epoch 1 Batch 354 Loss 0.0008\n",
      "Epoch 1 Batch 355 Loss 0.0016\n",
      "Epoch 1 Batch 356 Loss 0.0014\n",
      "Epoch 1 Batch 357 Loss 0.0007\n",
      "Epoch 1 Batch 358 Loss 0.0012\n",
      "Epoch 1 Batch 359 Loss 0.0010\n",
      "Epoch 1 Batch 360 Loss 0.0008\n",
      "Epoch 1 Batch 361 Loss 0.0012\n",
      "Epoch 1 Batch 362 Loss 0.0008\n",
      "Epoch 1 Batch 363 Loss 0.0010\n",
      "Epoch 1 Batch 364 Loss 0.0009\n",
      "Epoch 1 Batch 365 Loss 0.0014\n",
      "Epoch 1 Batch 366 Loss 0.0009\n",
      "Epoch 1 Batch 367 Loss 0.0008\n",
      "Epoch 1 Batch 368 Loss 0.0008\n",
      "Epoch 1 Batch 369 Loss 0.0007\n",
      "Epoch 1 Batch 370 Loss 0.0011\n",
      "Epoch 1 Batch 371 Loss 0.0007\n",
      "Epoch 1 Batch 372 Loss 0.0008\n",
      "Epoch 1 Batch 373 Loss 0.0007\n",
      "Epoch 1 Batch 374 Loss 0.0012\n",
      "Epoch 1 Batch 375 Loss 0.0006\n",
      "Epoch 1 Batch 376 Loss 0.0008\n",
      "Epoch 1 Batch 377 Loss 0.0010\n",
      "Epoch 1 Batch 378 Loss 0.0010\n",
      "Epoch 1 Batch 379 Loss 0.0008\n",
      "Epoch 1 Batch 380 Loss 0.0007\n",
      "Epoch 1 Batch 381 Loss 0.0010\n",
      "Epoch 1 Batch 382 Loss 0.0008\n",
      "Epoch 1 Batch 383 Loss 0.0010\n",
      "Epoch 1 Batch 384 Loss 0.0011\n",
      "Epoch 1 Batch 385 Loss 0.0008\n",
      "Epoch 1 Batch 386 Loss 0.0009\n",
      "Epoch 1 Batch 387 Loss 0.0007\n",
      "Epoch 1 Batch 388 Loss 0.0007\n",
      "Epoch 1 Batch 389 Loss 0.0007\n",
      "Epoch 1 Batch 390 Loss 0.0008\n",
      "Epoch 1 Batch 391 Loss 0.0009\n",
      "Epoch 1 Batch 392 Loss 0.0017\n",
      "Epoch 1 Batch 393 Loss 0.0007\n",
      "Epoch 1 Batch 394 Loss 0.0012\n",
      "Epoch 1 Batch 395 Loss 0.0009\n",
      "Epoch 1 Batch 396 Loss 0.0005\n",
      "Epoch 1 Batch 397 Loss 0.0009\n",
      "Epoch 1 Batch 398 Loss 0.0007\n",
      "Epoch 1 Batch 399 Loss 0.0009\n",
      "Epoch 1 Batch 400 Loss 0.0007\n",
      "Epoch 1 Batch 401 Loss 0.0010\n",
      "Epoch 1 Batch 402 Loss 0.0010\n",
      "Epoch 1 Batch 403 Loss 0.0007\n",
      "Epoch 1 Batch 404 Loss 0.0012\n",
      "Epoch 1 Batch 405 Loss 0.0007\n",
      "Epoch 1 Batch 406 Loss 0.0010\n",
      "Epoch 1 Batch 407 Loss 0.0008\n",
      "Epoch 1 Batch 408 Loss 0.0010\n",
      "Epoch 1 Batch 409 Loss 0.0008\n",
      "Epoch 1 Batch 410 Loss 0.0011\n",
      "Epoch 1 Batch 411 Loss 0.0009\n",
      "Epoch 1 Batch 412 Loss 0.0009\n",
      "Epoch 1 Batch 413 Loss 0.0010\n",
      "Epoch 1 Batch 414 Loss 0.0008\n",
      "Epoch 1 Batch 415 Loss 0.0006\n",
      "Epoch 1 Batch 416 Loss 0.0009\n",
      "Epoch 1 Batch 417 Loss 0.0010\n",
      "Epoch 1 Batch 418 Loss 0.0011\n",
      "Epoch 1 Batch 419 Loss 0.0008\n",
      "Epoch 1 Batch 420 Loss 0.0008\n",
      "Epoch 1 Batch 421 Loss 0.0008\n",
      "Epoch 1 Batch 422 Loss 0.0008\n",
      "Epoch 1 Batch 423 Loss 0.0009\n",
      "Epoch 1 Batch 424 Loss 0.0007\n",
      "Epoch 1 Batch 425 Loss 0.0006\n",
      "Epoch 1 Batch 426 Loss 0.0010\n",
      "Epoch 1 Batch 427 Loss 0.0010\n",
      "Epoch 1 Batch 428 Loss 0.0012\n",
      "Epoch 1 Batch 429 Loss 0.0007\n",
      "Epoch 1 Batch 430 Loss 0.0006\n",
      "Epoch 1 Batch 431 Loss 0.0007\n",
      "Epoch 1 Batch 432 Loss 0.0006\n",
      "Epoch 1 Batch 433 Loss 0.0008\n",
      "Epoch 1 Batch 434 Loss 0.0010\n",
      "Epoch 1 Batch 435 Loss 0.0008\n",
      "Epoch 1 Batch 436 Loss 0.0007\n",
      "Epoch 1 Batch 437 Loss 0.0006\n",
      "Epoch 1 Batch 438 Loss 0.0007\n",
      "Epoch 1 Batch 439 Loss 0.0006\n",
      "Epoch 1 Batch 440 Loss 0.0008\n",
      "Epoch 1 Batch 441 Loss 0.0006\n",
      "Epoch 1 Batch 442 Loss 0.0007\n",
      "Epoch 1 Batch 443 Loss 0.0027\n",
      "Epoch 1 Batch 444 Loss 0.0007\n",
      "Epoch 1 Batch 445 Loss 0.0007\n",
      "Epoch 1 Batch 446 Loss 0.0010\n",
      "Epoch 1 Batch 447 Loss 0.0006\n",
      "Epoch 1 Batch 448 Loss 0.0008\n",
      "Epoch 1 Batch 449 Loss 0.0009\n",
      "Epoch 1 Batch 450 Loss 0.0007\n",
      "Epoch 1 Batch 451 Loss 0.0008\n",
      "Epoch 1 Batch 452 Loss 0.0008\n",
      "Epoch 1 Batch 453 Loss 0.0005\n",
      "Epoch 1 Batch 454 Loss 0.0005\n",
      "Epoch 1 Batch 455 Loss 0.0006\n",
      "Epoch 1 Batch 456 Loss 0.0005\n",
      "Epoch 1 Batch 457 Loss 0.0011\n",
      "Epoch 1 Batch 458 Loss 0.0011\n",
      "Epoch 1 Batch 459 Loss 0.0006\n",
      "Epoch 1 Batch 460 Loss 0.0008\n",
      "Epoch 1 Batch 461 Loss 0.0011\n",
      "Epoch 1 Batch 462 Loss 0.0011\n",
      "Epoch 1 Batch 463 Loss 0.0006\n",
      "Epoch 1 Batch 464 Loss 0.0012\n",
      "Epoch 1 Batch 465 Loss 0.0006\n",
      "Epoch 1 Batch 466 Loss 0.0006\n",
      "Epoch 1 Batch 467 Loss 0.0014\n",
      "Epoch 1 Batch 468 Loss 0.0008\n",
      "Epoch 1 Batch 469 Loss 0.0010\n",
      "Epoch 1 Batch 470 Loss 0.0013\n",
      "Epoch 1 Batch 471 Loss 0.0009\n",
      "Epoch 1 Batch 472 Loss 0.0009\n",
      "Epoch 1 Batch 473 Loss 0.0007\n",
      "Epoch 1 Batch 474 Loss 0.0007\n",
      "Epoch 1 Batch 475 Loss 0.0009\n",
      "Epoch 1 Batch 476 Loss 0.0007\n",
      "Epoch 1 Batch 477 Loss 0.0010\n",
      "Epoch 1 Batch 478 Loss 0.0007\n",
      "Epoch 1 Batch 479 Loss 0.0008\n",
      "Epoch 1 Batch 480 Loss 0.0007\n",
      "Epoch 1 Batch 481 Loss 0.0005\n",
      "Epoch 1 Batch 482 Loss 0.0006\n",
      "Epoch 1 Batch 483 Loss 0.0009\n",
      "Epoch 1 Batch 484 Loss 0.0006\n",
      "Epoch 1 Batch 485 Loss 0.0006\n",
      "Epoch 1 Batch 486 Loss 0.0010\n",
      "Epoch 1 Batch 487 Loss 0.0007\n",
      "Epoch 1 Batch 488 Loss 0.0007\n",
      "Epoch 1 Batch 489 Loss 0.0010\n",
      "Epoch 1 Batch 490 Loss 0.0010\n",
      "Epoch 1 Batch 491 Loss 0.0010\n",
      "Epoch 1 Batch 492 Loss 0.0007\n",
      "Epoch 1 Batch 493 Loss 0.0006\n",
      "Epoch 1 Batch 494 Loss 0.0006\n",
      "Epoch 1 Batch 495 Loss 0.0009\n",
      "Epoch 1 Batch 496 Loss 0.0008\n",
      "Epoch 1 Batch 497 Loss 0.0005\n",
      "Epoch 1 Batch 498 Loss 0.0011\n",
      "Epoch 1 Batch 499 Loss 0.0008\n",
      "Epoch 1 Batch 500 Loss 0.0005\n",
      "Epoch 1 Batch 501 Loss 0.0009\n",
      "Epoch 1 Batch 502 Loss 0.0010\n",
      "Epoch 1 Batch 503 Loss 0.0007\n",
      "Epoch 1 Batch 504 Loss 0.0009\n",
      "Epoch 1 Batch 505 Loss 0.0010\n",
      "Epoch 1 Batch 506 Loss 0.0005\n",
      "Epoch 1 Batch 507 Loss 0.0006\n",
      "Epoch 1 Batch 508 Loss 0.0009\n",
      "Epoch 1 Batch 509 Loss 0.0007\n",
      "Epoch 1 Batch 510 Loss 0.0006\n",
      "Epoch 1 Batch 511 Loss 0.0010\n",
      "Epoch 1 Batch 512 Loss 0.0007\n",
      "Epoch 1 Batch 513 Loss 0.0008\n",
      "Epoch 1 Batch 514 Loss 0.0007\n",
      "Epoch 1 Batch 515 Loss 0.0005\n",
      "Epoch 1 Batch 516 Loss 0.0007\n",
      "Epoch 1 Batch 517 Loss 0.0006\n",
      "Epoch 1 Batch 518 Loss 0.0006\n",
      "Epoch 1 Batch 519 Loss 0.0007\n",
      "Epoch 1 Batch 520 Loss 0.0005\n",
      "Epoch 1 Batch 521 Loss 0.0006\n",
      "Epoch 1 Batch 522 Loss 0.0005\n",
      "Epoch 1 Batch 523 Loss 0.0005\n",
      "Epoch 1 Batch 524 Loss 0.0005\n",
      "Epoch 1 Batch 525 Loss 0.0006\n",
      "Epoch 1 Batch 526 Loss 0.0006\n",
      "Epoch 1 Batch 527 Loss 0.0007\n",
      "Epoch 1 Batch 528 Loss 0.0007\n",
      "Epoch 1 Batch 529 Loss 0.0007\n",
      "Epoch 1 Batch 530 Loss 0.0006\n",
      "Epoch 1 Batch 531 Loss 0.0006\n",
      "Epoch 1 Batch 532 Loss 0.0006\n",
      "Epoch 1 Batch 533 Loss 0.0006\n",
      "Epoch 1 Batch 534 Loss 0.0006\n",
      "Epoch 1 Batch 535 Loss 0.0005\n",
      "Epoch 1 Batch 536 Loss 0.0006\n",
      "Epoch 1 Batch 537 Loss 0.0007\n",
      "Epoch 1 Batch 538 Loss 0.0005\n",
      "Epoch 1 Batch 539 Loss 0.0005\n",
      "Epoch 1 Batch 540 Loss 0.0005\n",
      "Epoch 1 Batch 541 Loss 0.0006\n",
      "Epoch 1 Batch 542 Loss 0.0008\n",
      "Epoch 1 Batch 543 Loss 0.0008\n",
      "Epoch 1 Batch 544 Loss 0.0006\n",
      "Epoch 1 Batch 545 Loss 0.0009\n",
      "Epoch 1 Batch 546 Loss 0.0008\n",
      "Epoch 1 Batch 547 Loss 0.0006\n",
      "Epoch 1 Batch 548 Loss 0.0007\n",
      "Epoch 1 Batch 549 Loss 0.0005\n",
      "Epoch 1 Batch 550 Loss 0.0007\n",
      "Epoch 1 Batch 551 Loss 0.0006\n",
      "Epoch 1 Batch 552 Loss 0.0007\n",
      "Epoch 1 Batch 553 Loss 0.0005\n",
      "Epoch 1 Batch 554 Loss 0.0007\n",
      "Epoch 1 Batch 555 Loss 0.0006\n",
      "Epoch 1 Batch 556 Loss 0.0006\n",
      "Epoch 1 Batch 557 Loss 0.0005\n",
      "Epoch 1 Batch 558 Loss 0.0005\n",
      "Epoch 1 Batch 559 Loss 0.0010\n",
      "Epoch 1 Batch 560 Loss 0.0006\n",
      "Epoch 1 Batch 561 Loss 0.0005\n",
      "Epoch 1 Batch 562 Loss 0.0006\n",
      "Epoch 1 Batch 563 Loss 0.0006\n",
      "Epoch 1 Batch 564 Loss 0.0006\n",
      "Epoch 1 Batch 565 Loss 0.0009\n",
      "Epoch 1 Batch 566 Loss 0.0023\n",
      "Epoch 1 Batch 567 Loss 0.0008\n",
      "Epoch 1 Batch 568 Loss 0.0019\n",
      "Epoch 1 Batch 569 Loss 0.0006\n",
      "Epoch 1 Batch 570 Loss 0.0005\n",
      "Epoch 1 Batch 571 Loss 0.0013\n",
      "Epoch 1 Batch 572 Loss 0.0007\n",
      "Epoch 1 Batch 573 Loss 0.0006\n",
      "Epoch 1 Batch 574 Loss 0.0010\n",
      "Epoch 1 Batch 575 Loss 0.0006\n",
      "Epoch 1 Batch 576 Loss 0.0006\n",
      "Epoch 1 Batch 577 Loss 0.0006\n",
      "Epoch 1 Batch 578 Loss 0.0007\n",
      "Epoch 1 Batch 579 Loss 0.0087\n",
      "Epoch 1 Batch 580 Loss 0.0051\n",
      "Epoch 1 Batch 581 Loss 0.0007\n",
      "Epoch 1 Batch 582 Loss 0.0007\n",
      "Epoch 1 Batch 583 Loss 0.0008\n",
      "Epoch 1 Batch 584 Loss 0.0008\n",
      "Epoch 1 Batch 585 Loss 0.0056\n",
      "Epoch 1 Batch 586 Loss 0.0007\n",
      "Epoch 1 Batch 587 Loss 0.0009\n",
      "Epoch 1 Batch 588 Loss 0.0037\n",
      "Epoch 1 Batch 589 Loss 0.0006\n",
      "Epoch 1 Batch 590 Loss 0.0008\n",
      "Epoch 1 Batch 591 Loss 0.0007\n",
      "Epoch 1 Batch 592 Loss 0.0010\n",
      "Epoch 1 Batch 593 Loss 0.0008\n",
      "Epoch 1 Batch 594 Loss 0.0007\n",
      "Epoch 1 Batch 595 Loss 0.0007\n",
      "Epoch 1 Batch 596 Loss 0.0026\n",
      "Epoch 1 Batch 597 Loss 0.0007\n",
      "Epoch 1 Batch 598 Loss 0.0005\n",
      "Epoch 1 Batch 599 Loss 0.0006\n",
      "Epoch 1 Batch 600 Loss 0.0007\n",
      "Epoch 1 Batch 601 Loss 0.0005\n",
      "Epoch 1 Batch 602 Loss 0.0007\n",
      "Epoch 1 Batch 603 Loss 0.0008\n",
      "Epoch 1 Batch 604 Loss 0.0007\n",
      "Epoch 1 Batch 605 Loss 0.0007\n",
      "Epoch 1 Batch 606 Loss 0.0005\n",
      "Epoch 1 Batch 607 Loss 0.0007\n",
      "Epoch 1 Batch 608 Loss 0.0007\n",
      "Epoch 1 Batch 609 Loss 0.0005\n",
      "Epoch 1 Batch 610 Loss 0.0005\n",
      "Epoch 1 Batch 611 Loss 0.0005\n",
      "Epoch 1 Batch 612 Loss 0.0007\n",
      "Epoch 1 Batch 613 Loss 0.0004\n",
      "Epoch 1 Batch 614 Loss 0.0005\n",
      "Epoch 1 Batch 615 Loss 0.0007\n",
      "Epoch 1 Batch 616 Loss 0.0007\n",
      "Epoch 1 Batch 617 Loss 0.0005\n",
      "Epoch 1 Batch 618 Loss 0.0009\n",
      "Epoch 1 Batch 619 Loss 0.0005\n",
      "Epoch 1 Batch 620 Loss 0.0005\n",
      "Epoch 1 Batch 621 Loss 0.0023\n",
      "Epoch 1 Batch 622 Loss 0.0006\n",
      "Epoch 1 Batch 623 Loss 0.0006\n",
      "Epoch 1 Batch 624 Loss 0.0035\n",
      "Epoch 1 Batch 625 Loss 0.0017\n",
      "Epoch 1 Batch 626 Loss 0.0008\n",
      "Epoch 1 Batch 627 Loss 0.0010\n",
      "Epoch 1 Batch 628 Loss 0.0010\n",
      "Epoch 1 Batch 629 Loss 0.0006\n",
      "Epoch 1 Batch 630 Loss 0.0055\n",
      "Epoch 1 Batch 631 Loss 0.0113\n",
      "Epoch 1 Batch 632 Loss 0.0007\n",
      "Epoch 1 Batch 633 Loss 0.0007\n",
      "Epoch 1 Batch 634 Loss 0.0012\n",
      "Epoch 1 Batch 635 Loss 0.0050\n",
      "Epoch 1 Batch 636 Loss 0.0013\n",
      "Epoch 1 Batch 637 Loss 0.0009\n",
      "Epoch 1 Batch 638 Loss 0.0007\n",
      "Epoch 1 Batch 639 Loss 0.0058\n",
      "Epoch 1 Batch 640 Loss 0.0006\n",
      "Epoch 1 Batch 641 Loss 0.0008\n",
      "Epoch 1 Batch 642 Loss 0.0009\n",
      "Epoch 1 Batch 643 Loss 0.0009\n",
      "Epoch 1 Batch 644 Loss 0.0032\n",
      "Epoch 1 Batch 645 Loss 0.0005\n",
      "Epoch 1 Batch 646 Loss 0.0005\n",
      "Epoch 1 Batch 647 Loss 0.0005\n",
      "Epoch 1 Batch 648 Loss 0.0004\n",
      "Epoch 1 Batch 649 Loss 0.0007\n",
      "Epoch 1 Batch 650 Loss 0.0005\n",
      "Epoch 1 Batch 651 Loss 0.0004\n",
      "Epoch 1 Batch 652 Loss 0.0005\n",
      "Epoch 1 Batch 653 Loss 0.0005\n",
      "Epoch 1 Batch 654 Loss 0.0005\n",
      "Epoch 1 Batch 655 Loss 0.0004\n",
      "Epoch 1 Batch 656 Loss 0.0009\n",
      "Epoch 1 Batch 657 Loss 0.0007\n",
      "Epoch 1 Batch 658 Loss 0.0006\n",
      "Epoch 1 Batch 659 Loss 0.0018\n",
      "Epoch 1 Batch 660 Loss 0.0007\n",
      "Epoch 1 Batch 661 Loss 0.0005\n",
      "Epoch 1 Batch 662 Loss 0.0005\n",
      "Epoch 1 Batch 663 Loss 0.0005\n",
      "Epoch 1 Batch 664 Loss 0.0006\n",
      "Epoch 1 Batch 665 Loss 0.0005\n",
      "Epoch 1 Batch 666 Loss 0.0007\n",
      "Epoch 1 Batch 667 Loss 0.0003\n",
      "Epoch 1 Batch 668 Loss 0.0003\n",
      "Epoch 1 Batch 669 Loss 0.0005\n",
      "Epoch 1 Batch 670 Loss 0.0004\n",
      "Epoch 1 Batch 671 Loss 0.0005\n",
      "Epoch 1 Batch 672 Loss 0.0004\n",
      "Epoch 1 Batch 673 Loss 0.0003\n",
      "Epoch 1 Batch 674 Loss 0.0005\n",
      "Epoch 1 Batch 675 Loss 0.0004\n",
      "Epoch 1 Batch 676 Loss 0.0004\n",
      "Epoch 1 Batch 677 Loss 0.0005\n",
      "Epoch 1 Batch 678 Loss 0.0007\n",
      "Epoch 1 Batch 679 Loss 0.0004\n",
      "Epoch 1 Batch 680 Loss 0.0007\n",
      "Epoch 1 Batch 681 Loss 0.0004\n",
      "Epoch 1 Batch 682 Loss 0.0004\n",
      "Epoch 1 Batch 683 Loss 0.0004\n",
      "Epoch 1 Batch 684 Loss 0.0004\n",
      "Epoch 1 Batch 685 Loss 0.0004\n",
      "Epoch 1 Batch 686 Loss 0.0003\n",
      "Epoch 1 Batch 687 Loss 0.0004\n",
      "Epoch 1 Batch 688 Loss 0.0004\n",
      "Epoch 1 Batch 689 Loss 0.0004\n",
      "Epoch 1 Batch 690 Loss 0.0004\n",
      "Epoch 1 Batch 691 Loss 0.0004\n",
      "Epoch 1 Batch 692 Loss 0.0005\n",
      "Epoch 1 Batch 693 Loss 0.0004\n",
      "Epoch 1 Batch 694 Loss 0.0004\n",
      "Epoch 1 Batch 695 Loss 0.0006\n",
      "Epoch 1 Batch 696 Loss 0.0009\n",
      "Epoch 1 Batch 697 Loss 0.0004\n",
      "Epoch 1 Batch 698 Loss 0.0005\n",
      "Epoch 1 Batch 699 Loss 0.0005\n",
      "Epoch 1 Batch 700 Loss 0.0005\n",
      "Epoch 1 Batch 701 Loss 0.0004\n",
      "Epoch 1 Batch 702 Loss 0.0006\n",
      "Epoch 1 Batch 703 Loss 0.0004\n",
      "Epoch 1 Batch 704 Loss 0.0004\n",
      "Epoch 1 Batch 705 Loss 0.0004\n",
      "Epoch 1 Batch 706 Loss 0.0004\n",
      "Epoch 1 Batch 707 Loss 0.0007\n",
      "Epoch 1 Batch 708 Loss 0.0005\n",
      "Epoch 1 Batch 709 Loss 0.0005\n",
      "Epoch 1 Batch 710 Loss 0.0004\n",
      "Epoch 1 Batch 711 Loss 0.0004\n",
      "Epoch 1 Batch 712 Loss 0.0003\n",
      "Epoch 1 Batch 713 Loss 0.0004\n",
      "Epoch 1 Batch 714 Loss 0.0004\n",
      "Epoch 1 Batch 715 Loss 0.0004\n",
      "Epoch 1 Batch 716 Loss 0.0005\n",
      "Epoch 1 Batch 717 Loss 0.0004\n",
      "Epoch 1 Batch 718 Loss 0.0004\n",
      "Epoch 1 Batch 719 Loss 0.0006\n",
      "Epoch 1 Batch 720 Loss 0.0003\n",
      "Epoch 1 Batch 721 Loss 0.0003\n",
      "Epoch 1 Batch 722 Loss 0.0004\n",
      "Epoch 1 Batch 723 Loss 0.0003\n",
      "Epoch 1 Batch 724 Loss 0.0003\n",
      "Epoch 1 Batch 725 Loss 0.0003\n",
      "Epoch 1 Batch 726 Loss 0.0004\n",
      "Epoch 1 Batch 727 Loss 0.0003\n",
      "Epoch 1 Batch 728 Loss 0.0003\n",
      "Epoch 1 Batch 729 Loss 0.0004\n",
      "Epoch 1 Batch 730 Loss 0.0005\n",
      "Epoch 1 Batch 731 Loss 0.0004\n",
      "Epoch 1 Batch 732 Loss 0.0005\n",
      "Epoch 1 Batch 733 Loss 0.0003\n",
      "Epoch 1 Batch 734 Loss 0.0003\n",
      "Epoch 1 Batch 735 Loss 0.0004\n",
      "Epoch 1 Batch 736 Loss 0.0003\n",
      "Epoch 1 Batch 737 Loss 0.0003\n",
      "Epoch 1 Batch 738 Loss 0.0004\n",
      "Epoch 1 Batch 739 Loss 0.0003\n",
      "Epoch 1 Batch 740 Loss 0.0003\n",
      "Epoch 1 Batch 741 Loss 0.0002\n",
      "Epoch 1 Batch 742 Loss 0.0003\n",
      "Epoch 1 Batch 743 Loss 0.0003\n",
      "Epoch 1 Batch 744 Loss 0.0004\n",
      "Epoch 1 Batch 745 Loss 0.0003\n",
      "Epoch 1 Batch 746 Loss 0.0004\n",
      "Epoch 1 Batch 747 Loss 0.0004\n",
      "Epoch 1 Batch 748 Loss 0.0004\n",
      "Epoch 1 Batch 749 Loss 0.0003\n",
      "Epoch 1 Batch 750 Loss 0.0003\n",
      "Epoch 1 Batch 751 Loss 0.0003\n",
      "Epoch 1 Batch 752 Loss 0.0003\n",
      "Epoch 1 Batch 753 Loss 0.0004\n",
      "Epoch 1 Batch 754 Loss 0.0003\n",
      "Epoch 1 Batch 755 Loss 0.0003\n",
      "Epoch 1 Batch 756 Loss 0.0003\n",
      "Epoch 1 Batch 757 Loss 0.0003\n",
      "Epoch 1 Batch 758 Loss 0.0004\n",
      "Epoch 1 Batch 759 Loss 0.0004\n",
      "Epoch 1 Batch 760 Loss 0.0003\n",
      "Epoch 1 Batch 761 Loss 0.0003\n",
      "Epoch 1 Batch 762 Loss 0.0004\n",
      "Epoch 1 Batch 763 Loss 0.0005\n",
      "Epoch 1 Batch 764 Loss 0.0011\n",
      "Epoch 1 Batch 765 Loss 0.0004\n",
      "Epoch 1 Batch 766 Loss 0.0004\n",
      "Epoch 1 Batch 767 Loss 0.0004\n",
      "Epoch 1 Batch 768 Loss 0.0004\n",
      "Epoch 1 Batch 769 Loss 0.0004\n",
      "Epoch 1 Batch 770 Loss 0.0003\n",
      "Epoch 1 Batch 771 Loss 0.0003\n",
      "Epoch 1 Batch 772 Loss 0.0003\n",
      "Epoch 1 Batch 773 Loss 0.0003\n",
      "Epoch 1 Batch 774 Loss 0.0004\n",
      "Epoch 1 Batch 775 Loss 0.0003\n",
      "Epoch 1 Batch 776 Loss 0.0003\n",
      "Epoch 1 Batch 777 Loss 0.0005\n",
      "Epoch 1 Batch 778 Loss 0.0002\n",
      "Epoch 1 Batch 779 Loss 0.0003\n",
      "Epoch 1 Batch 780 Loss 0.0004\n",
      "Epoch 1 Batch 781 Loss 0.0003\n",
      "Epoch 1 Batch 782 Loss 0.0005\n",
      "Epoch 1 Batch 783 Loss 0.0002\n",
      "Epoch 1 Batch 784 Loss 0.0002\n",
      "Epoch 1 Batch 785 Loss 0.0002\n",
      "Epoch 1 Batch 786 Loss 0.0003\n",
      "Epoch 1 Batch 787 Loss 0.0003\n",
      "Epoch 1 Batch 788 Loss 0.0004\n",
      "Epoch 1 Batch 789 Loss 0.0003\n",
      "Epoch 1 Batch 790 Loss 0.0004\n",
      "Epoch 1 Batch 791 Loss 0.0003\n",
      "Epoch 1 Batch 792 Loss 0.0003\n",
      "Epoch 1 Batch 793 Loss 0.0004\n",
      "Epoch 1 Batch 794 Loss 0.0003\n",
      "Epoch 1 Batch 795 Loss 0.0003\n",
      "Epoch 1 Batch 796 Loss 0.0003\n",
      "Epoch 1 Batch 797 Loss 0.0003\n",
      "Epoch 1 Batch 798 Loss 0.0004\n",
      "Epoch 1 Batch 799 Loss 0.0004\n",
      "Epoch 1 Batch 800 Loss 0.0004\n",
      "Epoch 1 Batch 801 Loss 0.0003\n",
      "Epoch 1 Batch 802 Loss 0.0004\n",
      "Epoch 1 Batch 803 Loss 0.0002\n",
      "Epoch 1 Batch 804 Loss 0.0003\n",
      "Epoch 1 Batch 805 Loss 0.0005\n",
      "Epoch 1 Batch 806 Loss 0.0004\n",
      "Epoch 1 Batch 807 Loss 0.0004\n",
      "Epoch 1 Batch 808 Loss 0.0003\n",
      "Epoch 1 Batch 809 Loss 0.0004\n",
      "Epoch 1 Batch 810 Loss 0.0008\n",
      "Epoch 1 Batch 811 Loss 0.0004\n",
      "Epoch 1 Batch 812 Loss 0.0004\n",
      "Epoch 1 Batch 813 Loss 0.0005\n",
      "Epoch 1 Batch 814 Loss 0.0004\n",
      "Epoch 1 Batch 815 Loss 0.0004\n",
      "Epoch 1 Batch 816 Loss 0.0005\n",
      "Epoch 1 Batch 817 Loss 0.0003\n",
      "Epoch 1 Batch 818 Loss 0.0005\n",
      "Epoch 1 Batch 819 Loss 0.0011\n",
      "Epoch 1 Batch 820 Loss 0.0004\n",
      "Epoch 1 Batch 821 Loss 0.0005\n",
      "Epoch 1 Batch 822 Loss 0.0004\n",
      "Epoch 1 Batch 823 Loss 0.0005\n",
      "Epoch 1 Batch 824 Loss 0.0005\n",
      "Epoch 1 Batch 825 Loss 0.0004\n",
      "Epoch 1 Batch 826 Loss 0.0003\n",
      "Epoch 1 Batch 827 Loss 0.0005\n",
      "Epoch 1 Batch 828 Loss 0.0003\n",
      "Epoch 1 Batch 829 Loss 0.0003\n",
      "Epoch 1 Batch 830 Loss 0.0003\n",
      "Epoch 1 Batch 831 Loss 0.0003\n",
      "Epoch 1 Batch 832 Loss 0.0003\n",
      "Epoch 1 Batch 833 Loss 0.0003\n",
      "Epoch 1 Batch 834 Loss 0.0002\n",
      "Epoch 1 Batch 835 Loss 0.0004\n",
      "Epoch 1 Batch 836 Loss 0.0003\n",
      "Epoch 1 Batch 837 Loss 0.0003\n",
      "Epoch 1 Batch 838 Loss 0.0004\n",
      "Epoch 1 Batch 839 Loss 0.0002\n",
      "Epoch 1 Batch 840 Loss 0.0003\n",
      "Epoch 1 Batch 841 Loss 0.0002\n",
      "Epoch 1 Batch 842 Loss 0.0004\n",
      "Epoch 1 Batch 843 Loss 0.0009\n",
      "Epoch 1 Batch 844 Loss 0.0003\n",
      "Epoch 1 Batch 845 Loss 0.0003\n",
      "Epoch 1 Batch 846 Loss 0.0003\n",
      "Epoch 1 Batch 847 Loss 0.0004\n",
      "Epoch 1 Batch 848 Loss 0.0003\n",
      "Epoch 1 Batch 849 Loss 0.0005\n",
      "Epoch 1 Batch 850 Loss 0.0002\n",
      "Epoch 1 Batch 851 Loss 0.0002\n",
      "Epoch 1 Batch 852 Loss 0.0003\n",
      "Epoch 1 Batch 853 Loss 0.0005\n",
      "Epoch 1 Batch 854 Loss 0.0009\n",
      "Epoch 1 Batch 855 Loss 0.0003\n",
      "Epoch 1 Batch 856 Loss 0.0003\n",
      "Epoch 1 Batch 857 Loss 0.0004\n",
      "Epoch 1 Batch 858 Loss 0.0002\n",
      "Epoch 1 Batch 859 Loss 0.0002\n",
      "Epoch 1 Batch 860 Loss 0.0003\n",
      "Epoch 1 Batch 861 Loss 0.0005\n",
      "Epoch 1 Batch 862 Loss 0.0003\n",
      "Epoch 1 Batch 863 Loss 0.0005\n",
      "Epoch 1 Batch 864 Loss 0.0004\n",
      "Epoch 1 Batch 865 Loss 0.0002\n",
      "Epoch 1 Batch 866 Loss 0.0004\n",
      "Epoch 1 Batch 867 Loss 0.0002\n",
      "Epoch 1 Batch 868 Loss 0.0002\n",
      "Epoch 1 Batch 869 Loss 0.0002\n",
      "Epoch 1 Batch 870 Loss 0.0003\n",
      "Epoch 1 Batch 871 Loss 0.0003\n",
      "Epoch 1 Batch 872 Loss 0.0004\n",
      "Epoch 1 Batch 873 Loss 0.0003\n",
      "Epoch 1 Batch 874 Loss 0.0002\n",
      "Epoch 1 Batch 875 Loss 0.0003\n",
      "Epoch 1 Batch 876 Loss 0.0003\n",
      "Epoch 1 Batch 877 Loss 0.0004\n",
      "Epoch 1 Batch 878 Loss 0.0002\n",
      "Epoch 1 Batch 879 Loss 0.0003\n",
      "Epoch 1 Batch 880 Loss 0.0002\n",
      "Epoch 1 Batch 881 Loss 0.0005\n",
      "Epoch 1 Batch 882 Loss 0.0003\n",
      "Epoch 1 Batch 883 Loss 0.0004\n",
      "Epoch 1 Batch 884 Loss 0.0003\n",
      "Epoch 1 Batch 885 Loss 0.0004\n",
      "Epoch 1 Batch 886 Loss 0.0003\n",
      "Epoch 1 Batch 887 Loss 0.0004\n",
      "Epoch 1 Batch 888 Loss 0.0003\n",
      "Epoch 1 Batch 889 Loss 0.0003\n",
      "Epoch 1 Batch 890 Loss 0.0003\n",
      "Epoch 1 Batch 891 Loss 0.0002\n",
      "Epoch 1 Batch 892 Loss 0.0003\n",
      "Epoch 1 Batch 893 Loss 0.0002\n",
      "Epoch 1 Batch 894 Loss 0.0003\n",
      "Epoch 1 Batch 895 Loss 0.0002\n",
      "Epoch 1 Batch 896 Loss 0.0002\n",
      "Epoch 1 Batch 897 Loss 0.0003\n",
      "Epoch 1 Batch 898 Loss 0.0005\n",
      "Epoch 1 Batch 899 Loss 0.0003\n",
      "Epoch 1 Batch 900 Loss 0.0002\n",
      "Epoch 1 Batch 901 Loss 0.0003\n",
      "Epoch 1 Batch 902 Loss 0.0002\n",
      "Epoch 1 Batch 903 Loss 0.0003\n",
      "Epoch 1 Batch 904 Loss 0.0003\n",
      "Epoch 1 Batch 905 Loss 0.0003\n",
      "Epoch 1 Batch 906 Loss 0.0002\n",
      "Epoch 1 Batch 907 Loss 0.0002\n",
      "Epoch 1 Batch 908 Loss 0.0003\n",
      "Epoch 1 Batch 909 Loss 0.0003\n",
      "Epoch 1 Batch 910 Loss 0.0003\n",
      "Epoch 1 Batch 911 Loss 0.0007\n",
      "Epoch 1 Batch 912 Loss 0.0007\n",
      "Epoch 1 Batch 913 Loss 0.0005\n",
      "Epoch 1 Batch 914 Loss 0.0003\n",
      "Epoch 1 Batch 915 Loss 0.0002\n",
      "Epoch 1 Batch 916 Loss 0.0003\n",
      "Epoch 1 Batch 917 Loss 0.0002\n",
      "Epoch 1 Batch 918 Loss 0.0003\n",
      "Epoch 1 Batch 919 Loss 0.0005\n",
      "Epoch 1 Batch 920 Loss 0.0003\n",
      "Epoch 1 Batch 921 Loss 0.0004\n",
      "Epoch 1 Batch 922 Loss 0.0005\n",
      "Epoch 1 Batch 923 Loss 0.0007\n",
      "Epoch 1 Batch 924 Loss 0.0005\n",
      "Epoch 1 Batch 925 Loss 0.0002\n",
      "Epoch 1 Batch 926 Loss 0.0004\n",
      "Epoch 1 Batch 927 Loss 0.0003\n",
      "Epoch 1 Batch 928 Loss 0.0003\n",
      "Epoch 1 Batch 929 Loss 0.0002\n",
      "Epoch 1 Batch 930 Loss 0.0003\n",
      "Epoch 1 Batch 931 Loss 0.0002\n",
      "Epoch 1 Batch 932 Loss 0.0003\n",
      "Epoch 1 Batch 933 Loss 0.0004\n",
      "Epoch 1 Batch 934 Loss 0.0003\n",
      "Epoch 1 Batch 935 Loss 0.0003\n",
      "Epoch 1 Batch 936 Loss 0.0003\n",
      "Epoch 1 Batch 937 Loss 0.0003\n",
      "Epoch 1 Batch 938 Loss 0.0004\n",
      "Epoch 1 Batch 939 Loss 0.0003\n",
      "Epoch 1 Batch 940 Loss 0.0004\n",
      "Epoch 1 Batch 941 Loss 0.0002\n",
      "Epoch 1 Batch 942 Loss 0.0003\n",
      "Epoch 1 Batch 943 Loss 0.0003\n",
      "Epoch 1 Batch 944 Loss 0.0003\n",
      "Epoch 1 Batch 945 Loss 0.0002\n",
      "Epoch 1 Batch 946 Loss 0.0002\n",
      "Epoch 1 Batch 947 Loss 0.0002\n",
      "Epoch 1 Batch 948 Loss 0.0002\n",
      "Epoch 1 Batch 949 Loss 0.0003\n",
      "Epoch 1 Batch 950 Loss 0.0002\n",
      "Epoch 1 Batch 951 Loss 0.0002\n",
      "Epoch 1 Batch 952 Loss 0.0002\n",
      "Epoch 1 Batch 953 Loss 0.0002\n",
      "Epoch 1 Batch 954 Loss 0.0003\n",
      "Epoch 1 Batch 955 Loss 0.0003\n",
      "Epoch 1 Batch 956 Loss 0.0003\n",
      "Epoch 1 Batch 957 Loss 0.0002\n",
      "Epoch 1 Batch 958 Loss 0.0003\n",
      "Epoch 1 Batch 959 Loss 0.0002\n",
      "Epoch 1 Batch 960 Loss 0.0002\n",
      "Epoch 1 Batch 961 Loss 0.0002\n",
      "Epoch 1 Batch 962 Loss 0.0002\n",
      "Epoch 1 Batch 963 Loss 0.0003\n",
      "Epoch 1 Batch 964 Loss 0.0004\n",
      "Epoch 1 Batch 965 Loss 0.0002\n",
      "Epoch 1 Batch 966 Loss 0.0004\n",
      "Epoch 1 Batch 967 Loss 0.0003\n",
      "Epoch 1 Batch 968 Loss 0.0003\n",
      "Epoch 1 Batch 969 Loss 0.0003\n",
      "Epoch 1 Batch 970 Loss 0.0003\n",
      "Epoch 1 Batch 971 Loss 0.0002\n",
      "Epoch 1 Batch 972 Loss 0.0003\n",
      "Epoch 1 Batch 973 Loss 0.0002\n",
      "Epoch 1 Batch 974 Loss 0.0003\n",
      "Epoch 1 Batch 975 Loss 0.0003\n",
      "Epoch 1 Batch 976 Loss 0.0002\n",
      "Epoch 1 Batch 977 Loss 0.0003\n",
      "Epoch 1 Batch 978 Loss 0.0002\n",
      "Epoch 1 Batch 979 Loss 0.0003\n",
      "Epoch 1 Batch 980 Loss 0.0004\n",
      "Epoch 1 Batch 981 Loss 0.0005\n",
      "Epoch 1 Batch 982 Loss 0.0004\n",
      "Epoch 1 Batch 983 Loss 0.0003\n",
      "Epoch 1 Batch 984 Loss 0.0003\n",
      "Epoch 1 Batch 985 Loss 0.0004\n",
      "Epoch 1 Batch 986 Loss 0.0002\n",
      "Epoch 1 Batch 987 Loss 0.0003\n",
      "Epoch 1 Batch 988 Loss 0.0002\n",
      "Epoch 1 Batch 989 Loss 0.0002\n",
      "Epoch 1 Batch 990 Loss 0.0002\n",
      "Epoch 1 Batch 991 Loss 0.0003\n",
      "Epoch 1 Batch 992 Loss 0.0002\n",
      "Epoch 1 Batch 993 Loss 0.0002\n",
      "Epoch 1 Batch 994 Loss 0.0003\n",
      "Epoch 1 Batch 995 Loss 0.0004\n",
      "Epoch 1 Batch 996 Loss 0.0002\n",
      "Epoch 1 Batch 997 Loss 0.0002\n",
      "Epoch 1 Batch 998 Loss 0.0003\n",
      "Epoch 1 Batch 999 Loss 0.0005\n",
      "Epoch 1 Batch 1000 Loss 0.0002\n"
     ]
    }
   ],
   "source": [
    "# with strategy.scope():\n",
    "counter=0\n",
    "total_loss=0\n",
    "for batch in ds:\n",
    "    if counter==1000:\n",
    "        break\n",
    "    counter+=1\n",
    "    _, t_loss=train_step(batch[0],batch[1])\n",
    "    total_loss+=t_loss\n",
    "    print ('Epoch {} Batch {} Loss {:.4f}'.format(1, counter, t_loss.numpy() ))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94dde229",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T17:43:31.532396Z",
     "iopub.status.busy": "2024-07-20T17:43:31.532018Z",
     "iopub.status.idle": "2024-07-20T17:43:35.381324Z",
     "shell.execute_reply": "2024-07-20T17:43:35.380181Z"
    },
    "papermill": {
     "duration": 3.92228,
     "end_time": "2024-07-20T17:43:35.383237",
     "exception": false,
     "start_time": "2024-07-20T17:43:31.460957",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.9997771, shape=(), dtype=float32)\n",
      "tf.Tensor(0.973842, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "file_names=[decode_image('/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/1000092795.jpg')]\n",
    "text=['Two young guys with shaggy hair look at their hands while hanging out in the yard .']\n",
    "a,b=transformer((file_names,text))\n",
    "print(tf.keras.losses.CosineSimilarity()(a,b))\n",
    "a,b=transformer((file_names,['.3']))\n",
    "print(tf.keras.losses.CosineSimilarity()(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e82769",
   "metadata": {
    "papermill": {
     "duration": 0.06949,
     "end_time": "2024-07-20T17:43:35.522286",
     "exception": false,
     "start_time": "2024-07-20T17:43:35.452796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 31296,
     "sourceId": 39911,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 838708,
     "sourceId": 1431853,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4602.349852,
   "end_time": "2024-07-20T17:43:38.935090",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-20T16:26:56.585238",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
